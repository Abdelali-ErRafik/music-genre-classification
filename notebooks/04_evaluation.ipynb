{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üéµ Classification des Genres Musicaux\n",
    "## Notebook 4: √âvaluation et Analyse Finale\n",
    "\n",
    "**Objectif:** Analyse approfondie des r√©sultats et pr√©paration des visualisations pour le rapport.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuration et Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "from pathlib import Path\n",
    "\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "from src.config import Config\n",
    "from src.models import ModelTrainer\n",
    "from src.evaluation import Evaluator\n",
    "from src.visualization import Visualizer\n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"‚úÖ Imports r√©ussis!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Charger les Donn√©es et Mod√®les"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger les features\n",
    "features_df = pd.read_csv(Config.DATA_PROCESSED / Config.FEATURES_FILE)\n",
    "print(f\"‚úÖ Features charg√©es: {len(features_df)} √©chantillons\")\n",
    "\n",
    "# Initialiser le trainer et pr√©parer les donn√©es\n",
    "trainer = ModelTrainer()\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = trainer.prepare_data(features_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger les mod√®les sauvegard√©s\n",
    "model_files = list(Config.MODELS_DIR.glob(\"*.joblib\"))\n",
    "print(f\"\\nMod√®les sauvegard√©s trouv√©s: {len(model_files)}\")\n",
    "\n",
    "for model_file in model_files:\n",
    "    model_name = trainer.load_model(model_file)\n",
    "\n",
    "# Ou r√©-entra√Æner si n√©cessaire\n",
    "if len(trainer.trained_models) == 0:\n",
    "    print(\"\\n‚ö†Ô∏è Aucun mod√®le sauvegard√©, r√©-entra√Ænement...\")\n",
    "    trainer.train_all_models(X_train, y_train, X_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. √âvaluation D√©taill√©e du Meilleur Mod√®le"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = Evaluator()\n",
    "visualizer = Visualizer()\n",
    "\n",
    "# Utiliser le premier mod√®le disponible comme \"meilleur\"\n",
    "best_model = list(trainer.trained_models.keys())[0]\n",
    "print(f\"\\nüèÜ Analyse du mod√®le: {best_model}\")\n",
    "\n",
    "# Pr√©dictions\n",
    "y_pred = trainer.predict(best_model, X_test)\n",
    "\n",
    "# M√©triques globales\n",
    "metrics = evaluator.calculate_metrics(y_test, y_pred)\n",
    "print(f\"\\nüìä M√©triques Globales:\")\n",
    "for name, value in metrics.items():\n",
    "    print(f\"   {name}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# M√©triques par classe\n",
    "class_metrics = evaluator.calculate_per_class_metrics(y_test, y_pred)\n",
    "print(\"\\nüìä M√©triques par Genre:\")\n",
    "print(class_metrics.round(3).to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Visualisations pour le Rapport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure 1: Matrice de confusion normalis√©e\n",
    "fig = evaluator.plot_confusion_matrix(\n",
    "    y_test, y_pred,\n",
    "    normalize=True,\n",
    "    title=f\"Matrice de Confusion Normalis√©e - {best_model}\",\n",
    "    save_name=\"confusion_matrix_normalized.png\"\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure 2: Matrice de confusion (valeurs absolues)\n",
    "fig = evaluator.plot_confusion_matrix(\n",
    "    y_test, y_pred,\n",
    "    normalize=False,\n",
    "    title=f\"Matrice de Confusion - {best_model}\",\n",
    "    save_name=\"confusion_matrix_absolute.png\"\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure 3: Rapport de classification\n",
    "fig = evaluator.plot_classification_report(\n",
    "    y_test, y_pred,\n",
    "    title=f\"Performance par Genre - {best_model}\",\n",
    "    save_name=\"classification_report.png\"\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure 4: Accuracy par genre\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "# Calculer l'accuracy par genre\n",
    "cm = evaluator.get_confusion_matrix(y_test, y_pred)\n",
    "accuracy_per_genre = cm.diagonal() / cm.sum(axis=1)\n",
    "\n",
    "colors = ['green' if acc > 0.7 else 'orange' if acc > 0.5 else 'red' \n",
    "          for acc in accuracy_per_genre]\n",
    "\n",
    "bars = ax.bar(Config.GENRES, accuracy_per_genre, color=colors)\n",
    "ax.axhline(y=0.7, color='green', linestyle='--', alpha=0.5, label='Seuil 70%')\n",
    "ax.axhline(y=np.mean(accuracy_per_genre), color='blue', linestyle='-', alpha=0.7, \n",
    "           label=f'Moyenne: {np.mean(accuracy_per_genre):.1%}')\n",
    "\n",
    "ax.set_xlabel('Genre')\n",
    "ax.set_ylabel('Accuracy')\n",
    "ax.set_title('Accuracy par Genre Musical', fontsize=14, fontweight='bold')\n",
    "ax.set_ylim(0, 1)\n",
    "ax.legend()\n",
    "\n",
    "for bar, acc in zip(bars, accuracy_per_genre):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02,\n",
    "            f'{acc:.1%}', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.savefig(Config.REPORTS_DIR / 'accuracy_per_genre.png', dpi=100, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Analyse des Erreurs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyser les erreurs\n",
    "confused_pairs = evaluator.get_most_confused_pairs(y_test, y_pred, top_n=10)\n",
    "\n",
    "print(\"\\n‚ö†Ô∏è Paires de genres les plus confondues:\")\n",
    "print(\"-\" * 40)\n",
    "for rank, (real, pred, count) in enumerate(confused_pairs, 1):\n",
    "    print(f\"   {rank}. {real} ‚Üí {pred}: {count} erreurs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation des erreurs\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "pairs = [f\"{r} ‚Üí {p}\" for r, p, c in confused_pairs[:8]]\n",
    "counts = [c for r, p, c in confused_pairs[:8]]\n",
    "\n",
    "colors = plt.cm.Reds(np.linspace(0.3, 0.8, len(pairs)))\n",
    "ax.barh(pairs, counts, color=colors)\n",
    "\n",
    "ax.set_xlabel('Nombre d\\'erreurs')\n",
    "ax.set_title('Paires de Genres les Plus Confondues', fontsize=14, fontweight='bold')\n",
    "\n",
    "for i, count in enumerate(counts):\n",
    "    ax.text(count + 0.2, i, str(count), va='center', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(Config.REPORTS_DIR / 'confused_pairs.png', dpi=100, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Importance des Features (si applicable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# V√©rifier si le mod√®le supporte l'importance des features\n",
    "importance_df = trainer.get_feature_importance(best_model)\n",
    "\n",
    "if importance_df is not None:\n",
    "    # Top 20 features\n",
    "    feature_cols = [c for c in features_df.columns if c not in ['filename', 'genre']]\n",
    "    importance_df['feature'] = feature_cols\n",
    "    top_features = importance_df.head(20)\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    ax.barh(top_features['feature'], top_features['importance'], color='steelblue')\n",
    "    ax.set_xlabel('Importance')\n",
    "    ax.set_title('Top 20 Caract√©ristiques les Plus Importantes', fontsize=14, fontweight='bold')\n",
    "    ax.invert_yaxis()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(Config.REPORTS_DIR / 'feature_importance.png', dpi=100, bbox_inches='tight')\n",
    "    plt.show()\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è {best_model} ne supporte pas l'importance des features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. R√©sum√© pour le Rapport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# G√©n√©rer le rapport textuel\n",
    "report = evaluator.generate_report(best_model, y_test, y_pred)\n",
    "print(report)\n",
    "\n",
    "# Sauvegarder le rapport\n",
    "with open(Config.REPORTS_DIR / 'evaluation_report.txt', 'w') as f:\n",
    "    f.write(report)\n",
    "print(f\"\\nüíæ Rapport sauvegard√©: {Config.REPORTS_DIR / 'evaluation_report.txt'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tableau r√©capitulatif\n",
    "summary = pd.DataFrame([\n",
    "    ['Dataset', 'GTZAN', '1000 fichiers'],\n",
    "    ['Genres', '10', 'Blues, Classical, Country, Disco, Hip-hop, Jazz, Metal, Pop, Reggae, Rock'],\n",
    "    ['Features extraites', str(len(feature_cols)), 'MFCC, Spectral, Chroma, Tempo, etc.'],\n",
    "    ['Meilleur mod√®le', best_model, ''],\n",
    "    ['Accuracy (Test)', f\"{metrics['accuracy']:.1%}\", ''],\n",
    "    ['F1-Score (Test)', f\"{metrics['f1_score']:.1%}\", ''],\n",
    "], columns=['M√©trique', 'Valeur', 'D√©tails'])\n",
    "\n",
    "print(\"\\nüìä TABLEAU R√âCAPITULATIF\")\n",
    "print(\"=\" * 60)\n",
    "print(summary.to_string(index=False))\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Sauvegarder\n",
    "summary.to_csv(Config.REPORTS_DIR / 'summary.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Conclusions et Recommandations\n",
    "\n",
    "### Points Forts:\n",
    "- [Compl√©ter selon vos r√©sultats]\n",
    "- Genres bien classifi√©s: ...\n",
    "\n",
    "### Points Faibles:\n",
    "- Genres confondus: ...\n",
    "- Limitations: ...\n",
    "\n",
    "### Recommandations:\n",
    "1. Utiliser des r√©seaux de neurones profonds (CNN sur spectrogrammes)\n",
    "2. Augmenter le dataset avec plus d'exemples\n",
    "3. Tester des techniques d'augmentation de donn√©es audio\n",
    "4. Explorer des features suppl√©mentaires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Liste des fichiers g√©n√©r√©s\n",
    "print(\"\\nüìÅ Fichiers g√©n√©r√©s dans reports/:\")\n",
    "for file in Config.REPORTS_DIR.glob('*'):\n",
    "    print(f\"   - {file.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"‚úÖ √âVALUATION TERMIN√âE!\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\"\"\n",
    "üìå Prochaines √©tapes:\n",
    "   1. R√©diger le rapport final avec les visualisations g√©n√©r√©es\n",
    "   2. Pr√©parer la pr√©sentation PowerPoint\n",
    "   3. S'entra√Æner pour la soutenance\n",
    "\n",
    "üìÖ Date de soutenance: Semaine du 23 f√©vrier 2026\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
