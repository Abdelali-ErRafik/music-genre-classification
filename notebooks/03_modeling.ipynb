{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üéµ Classification des Genres Musicaux\n",
    "## Notebook 3: Mod√©lisation et Entra√Ænement\n",
    "\n",
    "**Objectif:** Entra√Æner et comparer diff√©rents mod√®les de machine learning.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuration et Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "from src.config import Config\n",
    "from src.models import ModelTrainer\n",
    "from src.evaluation import Evaluator\n",
    "from src.visualization import Visualizer\n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "%matplotlib inline\n",
    "\n",
    "# Fixer la graine al√©atoire\n",
    "np.random.seed(Config.RANDOM_STATE)\n",
    "\n",
    "print(\"‚úÖ Imports r√©ussis!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Chargement des Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger les features extraites\n",
    "features_path = Config.DATA_PROCESSED / Config.FEATURES_FILE\n",
    "\n",
    "if features_path.exists():\n",
    "    features_df = pd.read_csv(features_path)\n",
    "    print(f\"‚úÖ Features charg√©es: {len(features_df)} fichiers, {len(features_df.columns)-2} caract√©ristiques\")\n",
    "else:\n",
    "    print(f\"‚ùå Fichier non trouv√©: {features_path}\")\n",
    "    print(\"   Ex√©cutez d'abord le notebook 02_feature_extraction.ipynb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Pr√©paration des Donn√©es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialiser le trainer\n",
    "trainer = ModelTrainer()\n",
    "\n",
    "# Pr√©parer les donn√©es\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = trainer.prepare_data(features_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Afficher les shapes\n",
    "print(f\"\\nüìä Dimensions des donn√©es:\")\n",
    "print(f\"   X_train: {X_train.shape}\")\n",
    "print(f\"   X_val:   {X_val.shape}\")\n",
    "print(f\"   X_test:  {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Mod√®les Disponibles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Afficher les mod√®les disponibles\n",
    "trainer.print_models_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Entra√Ænement de Tous les Mod√®les"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entra√Æner tous les mod√®les\n",
    "results_df = trainer.train_all_models(X_train, y_train, X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Afficher les r√©sultats\n",
    "print(\"\\nüìä R√©sultats de l'entra√Ænement:\")\n",
    "print(results_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualiser les r√©sultats\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "x = np.arange(len(results_df))\n",
    "width = 0.35\n",
    "\n",
    "bars1 = ax.bar(x - width/2, results_df['train_accuracy'], width, label='Train', color='steelblue')\n",
    "bars2 = ax.bar(x + width/2, results_df['val_accuracy'], width, label='Validation', color='coral')\n",
    "\n",
    "ax.set_xlabel('Mod√®le')\n",
    "ax.set_ylabel('Accuracy')\n",
    "ax.set_title('Comparaison des Mod√®les', fontsize=14, fontweight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(results_df['model_name'], rotation=45, ha='right')\n",
    "ax.legend()\n",
    "ax.set_ylim(0, 1)\n",
    "ax.axhline(y=0.8, color='green', linestyle='--', alpha=0.5, label='80%')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combiner X_train et X_val pour la cross-validation\n",
    "X_combined = np.vstack([X_train, X_val])\n",
    "y_combined = np.concatenate([y_train, y_val])\n",
    "\n",
    "print(f\"Donn√©es combin√©es: {X_combined.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-validation sur les 3 meilleurs mod√®les\n",
    "top_models = results_df.head(3)['model_name'].tolist()\n",
    "print(f\"\\nüèÜ Top 3 mod√®les: {top_models}\")\n",
    "\n",
    "cv_results = []\n",
    "for model_name in top_models:\n",
    "    result = trainer.cross_validate(model_name, X_combined, y_combined, cv=5)\n",
    "    cv_results.append({\n",
    "        'model': model_name,\n",
    "        'cv_mean': result['cv_mean'],\n",
    "        'cv_std': result['cv_std']\n",
    "    })\n",
    "\n",
    "cv_df = pd.DataFrame(cv_results)\n",
    "print(\"\\nüìä R√©sultats Cross-Validation:\")\n",
    "print(cv_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Optimisation des Hyperparam√®tres (Optionnel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimiser le meilleur mod√®le (optionnel, peut prendre du temps)\n",
    "# D√©commentez pour ex√©cuter\n",
    "\n",
    "# best_model_name = trainer.best_model_name\n",
    "# print(f\"\\nüîß Optimisation de {best_model_name}...\")\n",
    "# tuning_results = trainer.hyperparameter_tuning(best_model_name, X_combined, y_combined)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. √âvaluation sur le Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# √âvaluer le meilleur mod√®le\n",
    "evaluator = Evaluator()\n",
    "\n",
    "best_model = trainer.best_model_name\n",
    "y_pred = trainer.predict(best_model, X_test)\n",
    "\n",
    "# Afficher le rapport\n",
    "evaluator.print_evaluation(best_model, y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrice de confusion\n",
    "evaluator.plot_confusion_matrix(\n",
    "    y_test, y_pred,\n",
    "    normalize=True,\n",
    "    title=f\"Matrice de Confusion - {best_model}\"\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rapport de classification visuel\n",
    "evaluator.plot_classification_report(\n",
    "    y_test, y_pred,\n",
    "    title=f\"Rapport de Classification - {best_model}\"\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Analyse des Erreurs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paires de genres les plus confondues\n",
    "confused_pairs = evaluator.get_most_confused_pairs(y_test, y_pred, top_n=5)\n",
    "\n",
    "print(\"\\n‚ö†Ô∏è Genres les plus souvent confondus:\")\n",
    "for real, pred, count in confused_pairs:\n",
    "    print(f\"   {real} ‚Üí {pred}: {count} erreurs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualiser les confusions\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "pairs = [f\"{r}‚Üí{p}\" for r, p, c in confused_pairs]\n",
    "counts = [c for r, p, c in confused_pairs]\n",
    "\n",
    "ax.barh(pairs, counts, color='coral')\n",
    "ax.set_xlabel('Nombre d\\'erreurs')\n",
    "ax.set_title('Paires de Genres les Plus Confondues', fontsize=14, fontweight='bold')\n",
    "\n",
    "for i, count in enumerate(counts):\n",
    "    ax.text(count + 0.1, i, str(count), va='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Comparaison Finale de Tous les Mod√®les"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# √âvaluer tous les mod√®les sur le test set\n",
    "final_results = []\n",
    "\n",
    "for model_name in trainer.trained_models.keys():\n",
    "    y_pred_model = trainer.predict(model_name, X_test)\n",
    "    metrics = evaluator.calculate_metrics(y_test, y_pred_model)\n",
    "    \n",
    "    final_results.append({\n",
    "        'Model': model_name,\n",
    "        'Accuracy': metrics['accuracy'],\n",
    "        'Precision': metrics['precision'],\n",
    "        'Recall': metrics['recall'],\n",
    "        'F1-Score': metrics['f1_score']\n",
    "    })\n",
    "\n",
    "final_df = pd.DataFrame(final_results)\n",
    "final_df = final_df.sort_values('Accuracy', ascending=False)\n",
    "\n",
    "print(\"\\nüìä R√©sultats Finaux sur le Test Set:\")\n",
    "print(final_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graphique de comparaison finale\n",
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "\n",
    "x = np.arange(len(final_df))\n",
    "width = 0.2\n",
    "\n",
    "metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score']\n",
    "colors = ['steelblue', 'coral', 'seagreen', 'purple']\n",
    "\n",
    "for i, (metric, color) in enumerate(zip(metrics, colors)):\n",
    "    offset = (i - 1.5) * width\n",
    "    ax.bar(x + offset, final_df[metric], width, label=metric, color=color, alpha=0.8)\n",
    "\n",
    "ax.set_xlabel('Mod√®le')\n",
    "ax.set_ylabel('Score')\n",
    "ax.set_title('Comparaison Finale des Mod√®les', fontsize=14, fontweight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(final_df['Model'], rotation=45, ha='right')\n",
    "ax.legend()\n",
    "ax.set_ylim(0, 1)\n",
    "ax.axhline(y=0.8, color='green', linestyle='--', alpha=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Sauvegarde du Meilleur Mod√®le"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sauvegarder le meilleur mod√®le\n",
    "save_path = trainer.save_model(trainer.best_model_name)\n",
    "print(f\"\\nüíæ Mod√®le sauvegard√©: {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sauvegarder les r√©sultats\n",
    "final_df.to_csv(Config.REPORTS_DIR / 'model_comparison.csv', index=False)\n",
    "print(f\"üìä R√©sultats sauvegard√©s: {Config.REPORTS_DIR / 'model_comparison.csv'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Conclusions\n",
    "\n",
    "### R√©sum√© des R√©sultats:\n",
    "\n",
    "- **Meilleur mod√®le:** [Nom du mod√®le]\n",
    "- **Accuracy:** XX%\n",
    "- **Genres bien classifi√©s:** Classical, Metal, ...\n",
    "- **Genres souvent confondus:** Rock/Metal, Jazz/Blues, ...\n",
    "\n",
    "### Interpr√©tation:\n",
    "- Les genres avec des caract√©ristiques acoustiques distinctes (classique, metal) sont mieux classifi√©s\n",
    "- Les genres proches (rock/metal, jazz/blues) sont plus difficiles √† distinguer\n",
    "\n",
    "### Pistes d'am√©lioration:\n",
    "1. Utiliser plus de donn√©es\n",
    "2. Essayer des r√©seaux de neurones profonds (CNN sur spectrogrammes)\n",
    "3. Feature engineering suppl√©mentaire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n‚úÖ Mod√©lisation termin√©e!\")\n",
    "print(f\"\\nüèÜ Meilleur mod√®le: {trainer.best_model_name}\")\n",
    "print(\"\\nüìå Passez au notebook 04_evaluation.ipynb pour une analyse plus d√©taill√©e.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
